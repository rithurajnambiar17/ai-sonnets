{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 10  # sequence length for each line of the sonnet\n",
    "BATCH_SIZE = 128  # batch size for training\n",
    "BUFFER_SIZE = 10000  # buffer size for shuffling the data\n",
    "EMBEDDING_DIM = 256  # embedding dimension for the generator and discriminator\n",
    "GEN_HIDDEN_DIM = 512  # number of hidden units in the generator\n",
    "DISC_HIDDEN_DIM = 512  # number of hidden units in the discriminator\n",
    "DROPOUT_RATE = 0.3  # dropout rate for the generator and discriminator\n",
    "LEARNING_RATE = 1e-4  # learning rate for the optimizer\n",
    "EPOCHS = 100  # number of epochs to train for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the sonnets from the file\n",
    "with open(\"./data/Sonnet.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# split the text into sonnets\n",
    "sonnets = re.split(\"\\n\\n\", text)\n",
    "\n",
    "# split the sonnets into lines\n",
    "lines = []\n",
    "for sonnet in sonnets:\n",
    "    sonnet_lines = sonnet.split(\"\\n\")\n",
    "    for line in sonnet_lines:\n",
    "        if line.strip() != \"\":\n",
    "            lines.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    # define the regex pattern for punctuation\n",
    "    pattern = r\"[^\\w\\s]\"\n",
    "\n",
    "    # remove all occurrences of the pattern from the text\n",
    "    text = re.sub(pattern, \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lines)):\n",
    "    lines[i] = remove_punctuation(lines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the words\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer.fit_on_texts(lines)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)\n",
    "\n",
    "# convert the lines to sequences of token IDs\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "# pad the sequences to a fixed length\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences, maxlen=9, padding=\"post\", truncating=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size + 1, EMBEDDING_DIM, input_shape=(SEQ_LENGTH,)))\n",
    "    model.add(tf.keras.layers.LSTM(GEN_HIDDEN_DIM, dropout=DROPOUT_RATE, return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(GEN_HIDDEN_DIM, dropout=DROPOUT_RATE))\n",
    "    model.add(tf.keras.layers.Dense(SEQ_LENGTH, activation=\"softmax\"))\n",
    "    return model\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size + 1, EMBEDDING_DIM, input_shape=(SEQ_LENGTH,)))\n",
    "    model.add(tf.keras.layers.LSTM(DISC_HIDDEN_DIM, dropout=DROPOUT_RATE, return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(DISC_HIDDEN_DIM, dropout=DROPOUT_RATE))\n",
    "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of hidden units for the generator and discriminator\n",
    "GEN_HIDDEN_UNITS = 128\n",
    "DISC_HIDDEN_UNITS = 64\n",
    "\n",
    "# define the learning rate for the optimizer\n",
    "LEARNING_RATE = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the generator model\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(GEN_HIDDEN_UNITS, input_shape=(SEQ_LENGTH,), activation='relu'),\n",
    "        tf.keras.layers.Dense(SEQ_LENGTH, activation='softmax')\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the discriminator model\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(DISC_HIDDEN_UNITS, input_shape=(SEQ_LENGTH,), activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the generator and discriminator models into a GAN\n",
    "def make_gan_model(generator, discriminator):\n",
    "    discriminator.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='binary_crossentropy')\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    gan = tf.keras.Sequential([\n",
    "        generator,\n",
    "        discriminator\n",
    "    ])\n",
    "\n",
    "    gan.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='binary_crossentropy')\n",
    "    return gan\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    " # create an array of real labels and fake labels for training the discriminator\n",
    "real_labels = np.ones((BATCH_SIZE, 1))\n",
    "fake_labels = np.zeros((BATCH_SIZE, 1))\n",
    "\n",
    "# create the generator, discriminator, and GAN models\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "gan = make_gan_model(generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_7\" is incompatible with the layer: expected shape=(None, 10), found shape=(128, 9)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m fake_samples \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mpredict(noise)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# train the discriminator on the real and fake sonnets\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m discriminator_loss_real \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m discriminator_loss_fake \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(fake_samples, fake_labels)\n\u001b[0;32m     13\u001b[0m discriminator_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39madd(discriminator_loss_real, discriminator_loss_fake)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py:2381\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2377\u001b[0m     iterator \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2378\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2379\u001b[0m     )\n\u001b[0;32m   2380\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 2381\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   2383\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2384\u001b[0m \u001b[39mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileimsgsqh1.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py:1146\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   1142\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[0;32m   1143\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m     )\n\u001b[0;32m   1145\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[1;32m-> 1146\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[0;32m   1147\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[0;32m   1148\u001b[0m     outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1149\u001b[0m )\n\u001b[0;32m   1150\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py:1135\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[1;32m-> 1135\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[0;32m   1136\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py:993\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[39m# Run forward pass.\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m--> 993\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(x, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    994\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(x, y, y_pred, sample_weight)\n\u001b[0;32m    995\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\input_spec.py:295\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39mif\u001b[39;00m spec_dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    294\u001b[0m     \u001b[39mif\u001b[39;00m spec_dim \u001b[39m!=\u001b[39m dim:\n\u001b[1;32m--> 295\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    296\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m is \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    297\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mincompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected shape=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    299\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfound shape=\u001b[39m\u001b[39m{\u001b[39;00mdisplay_shape(x\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    300\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_7\" is incompatible with the layer: expected shape=(None, 10), found shape=(128, 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train the GAN\n",
    "for epoch in range(100):\n",
    "    # select a random batch of real sonnets\n",
    "    real_samples = padded_sequences[np.random.randint(0, len(padded_sequences), size=BATCH_SIZE)]\n",
    "\n",
    "    # generate a batch of fake sonnets using the generator\n",
    "    noise = np.random.normal(0, 1, (32, SEQ_LENGTH))\n",
    "    fake_samples = generator.predict(noise)\n",
    "\n",
    "    # train the discriminator on the real and fake sonnets\n",
    "    discriminator_loss_real = discriminator.train_on_batch(real_samples, real_labels)\n",
    "    discriminator_loss_fake = discriminator.train_on_batch(fake_samples, fake_labels)\n",
    "    discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n",
    "\n",
    "    # generate a new batch of noise for the generator\n",
    "    noise = np.random.normal(0, 1, (BATCH_SIZE, SEQ_LENGTH))\n",
    "\n",
    "    # train the generator to fool the discriminator\n",
    "    generator_loss = gan.train_on_batch(noise, real_labels)\n",
    "\n",
    "    # print the loss values for the discriminator and generator\n",
    "    print(f\"Epoch {epoch+1}/{100}, Discriminator Loss: {discriminator_loss}, Generator Loss: {generator_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd2ccc147b25bc63e03b8c2078329f48b3831a7ce96f0169912c1ccdf48e1006"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
