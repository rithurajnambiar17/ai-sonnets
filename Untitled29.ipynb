{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7NYN6aGQJJx6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, Embedding, multiply\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2DTranspose, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the sonnets from the Sonnet.txt file\n",
        "sonnet_file = open('/content/Sonnet.txt', 'r')\n",
        "sonnets = sonnet_file.read().split('\\n\\n')\n",
        "sonnet_file.close()\n",
        "\n",
        "# Clean the sonnets by removing extra whitespaces and converting to lowercase\n",
        "cleaned_sonnets = []\n",
        "for sonnet in sonnets:\n",
        "    sonnet = sonnet.strip().lower()\n",
        "    sonnet = ' '.join(sonnet.split())\n",
        "    cleaned_sonnets.append(sonnet)\n",
        "\n",
        "# Preprocess the sonnets using the Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(cleaned_sonnets)\n",
        "sequences = tokenizer.texts_to_sequences(cleaned_sonnets)\n",
        "max_sequence_length = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=1000, padding='post')\n",
        "vocabulary_size = len(tokenizer.word_index) + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fKodnEJ5MEp5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_length = 1000\n",
        "# Define the generator model\n",
        "def build_generator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim=100))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(max_sequence_length * vocabulary_size, activation='softmax'))\n",
        "    model.add(Reshape((max_sequence_length, vocabulary_size)))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer = 'adam')\n",
        "    return model\n",
        "\n",
        "# Define the discriminator model\n",
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(max_sequence_length, vocabulary_size)))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "def build_combined(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    sequence_input = Input(shape=(100,))\n",
        "    generated_sequence = generator(sequence_input)\n",
        "    validity = discriminator(generated_sequence)\n",
        "    combined_model = Model(sequence_input, validity)\n",
        "    combined_model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "    return combined_model\n",
        "\n",
        "# Build the generator, discriminator, and combined models\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "combined = build_combined(generator, discriminator)\n",
        "\n",
        "# Train the GAN model\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "valid = np.ones((batch_size, 1))\n",
        "fake = np.zeros((batch_size, 1))\n"
      ],
      "metadata": {
        "id": "FrKBWl3RJ71S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    # Train discriminator on real sequences\n",
        "    sequence_indices = np.random.randint(0, padded_sequences.shape[0], batch_size)\n",
        "    real_sequences = padded_sequences[sequence_indices]\n",
        "    discriminator_loss_real = discriminator.train_on_batch(real_sequences, valid)\n",
        "\n",
        "    # Train discriminator on generated sequences\n",
        "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "    generated_sequences = generator.predict(noise)\n",
        "    discriminator_loss_fake = discriminator.train_on_batch(generated_sequences, fake)\n",
        "\n",
        "    # Train generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "    generator_loss = combined.train_on_batch(noise, valid)\n",
        "\n",
        "    # Print progress every 100 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch} - Discriminator loss on real sequences: {discriminator_loss_real} - Discriminator loss on generated sequences: {discriminator_loss_fake} - Generator loss: {generator_loss}\")\n"
      ],
      "metadata": {
        "id": "R0R-3fxZMv6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a sonnet using the trained generator\n",
        "noise = np.random.normal(0, 1, (1, 100))\n",
        "generated_sequence = generator.predict(noise)\n",
        "generated_sonnet = []\n",
        "for sequence in generated_sequence[0]:\n",
        "    index = np.argmax(sequence)\n",
        "    word = tokenizer.index_word[index]\n",
        "    if word is None:\n",
        "        break\n",
        "    generated_sonnet.append(word)\n",
        "generated_sonnet = ' '.join(generated_sonnet)\n",
        "print(generated_sonnet)\n"
      ],
      "metadata": {
        "id": "13RVPHn-JUf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the GAN model\n",
        "batch_size = 32\n",
        "epochs = 10000\n",
        "\n",
        "valid = np.ones((batch_size, 1))\n",
        "fake = np.zeros((batch_size, 1))"
      ],
      "metadata": {
        "id": "wni6icPHJk3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    # Train discriminator on real sequences\n",
        "    sequence_indices = np.random.randint(0, padded_sequences.shape[0], batch_size)\n",
        "    real_sequences = padded_sequences[sequence_indices]\n",
        "    discriminator_loss_real = discriminator.train_on_batch(real_sequences, valid)\n",
        "\n",
        "    # Train discriminator on generated sequences\n",
        "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "    generated_sequences = generator.predict(noise)\n",
        "    discriminator_loss_fake = discriminator.train_on_batch(generated_sequences, fake)\n",
        "\n",
        "    # Train generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "    generator_loss = combined.train_on_batch(noise, valid)\n",
        "\n",
        "    # Print progress every 100 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch} - Discriminator loss on real sequences: {discriminator_loss_real} - Discriminator loss on generated sequences: {discriminator_loss_fake} - Generator loss: {generator_loss}\")\n"
      ],
      "metadata": {
        "id": "EoSfC28AKUyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DNQOmrH0KWzm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}